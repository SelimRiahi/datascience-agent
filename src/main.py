# main.py - TON POINT DE D√âPART ! üöÄ
"""
Agent Data Scientist Autonome - Version D√©butant
Commencer par ex√©cuter ce fichier : python main.py
"""

import pandas as pd
import numpy as np
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# L'agent principal (version simplifi√©e pour d√©buter)
class SimpleAutonomousDataScientist:
    """Version simplifi√©e de l'agent pour d√©buter"""
    
    def __init__(self):
        self.data = None
        self.model = None
        self.results = {}
        print("ü§ñ Agent Data Scientist Autonome initialis√© !")
        print("=" * 50)
    
    def load_data(self, file_path):
        """Charger les donn√©es automatiquement"""
        print(f"üìä Chargement des donn√©es depuis {file_path}")
        
        try:
            if file_path.endswith('.csv'):
                self.data = pd.read_csv(file_path)
            elif file_path.endswith('.xlsx'):
                self.data = pd.read_excel(file_path)
            else:
                print("‚ùå Format non support√©. Utilise .csv ou .xlsx")
                return False
            
            print(f"‚úÖ Donn√©es charg√©es : {self.data.shape[0]} lignes, {self.data.shape[1]} colonnes")
            print(f"üìã Colonnes : {list(self.data.columns)}")
            return True
            
        except Exception as e:
            print(f"‚ùå Erreur lors du chargement : {e}")
            return False
    
    def analyze_data_quality(self):
        """Analyser automatiquement la qualit√© des donn√©es"""
        print("\nüîç ANALYSE AUTOMATIQUE DE LA QUALIT√â")
        print("-" * 40)
        
        if self.data is None:
            print("‚ùå Aucune donn√©e charg√©e !")
            return
        
        # Informations g√©n√©rales
        print(f"üìä Forme des donn√©es : {self.data.shape}")
        print(f"üíæ M√©moire utilis√©e : {self.data.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
        
        # Valeurs manquantes
        missing = self.data.isnull().sum()
        if missing.sum() > 0:
            print(f"\n‚ö†Ô∏è  Valeurs manquantes d√©tect√©es :")
            for col, count in missing[missing > 0].items():
                percentage = (count / len(self.data)) * 100
                print(f"   ‚Ä¢ {col}: {count} ({percentage:.1f}%)")
        else:
            print("‚úÖ Aucune valeur manquante !")
        
        # Types de donn√©es
        print(f"\nüìà Types de donn√©es :")
        for dtype, columns in self.data.dtypes.groupby(self.data.dtypes).items():
            print(f"   ‚Ä¢ {dtype}: {len(columns)} colonnes")
        
        # Doublons
        duplicates = self.data.duplicated().sum()
        if duplicates > 0:
            print(f"\nüîÑ Doublons : {duplicates} lignes")
        else:
            print("‚úÖ Aucun doublon d√©tect√©")
        
        # Statistiques descriptives
        print(f"\nüìä Aper√ßu des donn√©es :")
        print(self.data.head())
    
    def auto_clean_data(self):
        """Nettoyer automatiquement les donn√©es"""
        print("\nüßπ NETTOYAGE AUTOMATIQUE DES DONN√âES")
        print("-" * 40)
        
        if self.data is None:
            print("‚ùå Aucune donn√©e √† nettoyer !")
            return
        
        original_shape = self.data.shape
        
        # Supprimer les doublons
        before_dup = len(self.data)
        self.data = self.data.drop_duplicates()
        removed_dup = before_dup - len(self.data)
        if removed_dup > 0:
            print(f"‚úÖ {removed_dup} doublons supprim√©s")
        
        # G√©rer les valeurs manquantes
        for column in self.data.columns:
            if self.data[column].isnull().sum() > 0:
                if self.data[column].dtype == 'object':
                    # Variables cat√©gorielles : mode
                    mode_value = self.data[column].mode().iloc[0] if len(self.data[column].mode()) > 0 else 'Unknown'
                    self.data[column].fillna(mode_value, inplace=True)
                    print(f"‚úÖ {column}: valeurs manquantes ‚Üí '{mode_value}'")
                else:
                    # Variables num√©riques : m√©diane
                    median_value = self.data[column].median()
                    self.data[column].fillna(median_value, inplace=True)
                    print(f"‚úÖ {column}: valeurs manquantes ‚Üí {median_value:.2f}")
        
        print(f"üìä Forme finale : {original_shape} ‚Üí {self.data.shape}")
    
    def auto_model_selection(self, target_column):
        """S√©lectionner et entra√Æner automatiquement le meilleur mod√®le"""
        print(f"\nü§ñ S√âLECTION AUTOMATIQUE DU MOD√àLE")
        print("-" * 40)
        
        if self.data is None:
            print("‚ùå Aucune donn√©e disponible !")
            return
        
        if target_column not in self.data.columns:
            print(f"‚ùå Colonne '{target_column}' introuvable !")
            print(f"Colonnes disponibles : {list(self.data.columns)}")
            return
        
        # Pr√©parer les donn√©es
        from sklearn.model_selection import train_test_split
        from sklearn.preprocessing import LabelEncoder
        from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
        from sklearn.metrics import accuracy_score, mean_squared_error
        
        # S√©parer X et y
        X = self.data.drop(columns=[target_column])
        y = self.data[target_column]
        
        # Encoder les variables cat√©gorielles
        encoders = {}
        for column in X.columns:
            if X[column].dtype == 'object':
                encoders[column] = LabelEncoder()
                X[column] = encoders[column].fit_transform(X[column].astype(str))
        
        # D√©tecter le type de probl√®me
        if y.dtype == 'object' or y.nunique() < 10:
            problem_type = 'classification'
            if y.dtype == 'object':
                y_encoder = LabelEncoder()
                y = y_encoder.fit_transform(y)
        else:
            problem_type = 'regression'
        
        print(f"üéØ Type de probl√®me d√©tect√© : {problem_type}")
        print(f"üìä Features : {X.shape[1]}")
        print(f"üìà √âchantillons : {X.shape[0]}")
        
        # Division train/test
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # S√©lection et entra√Ænement du mod√®le
        if problem_type == 'classification':
            self.model = RandomForestClassifier(n_estimators=100, random_state=42)
            self.model.fit(X_train, y_train)
            y_pred = self.model.predict(X_test)
            score = accuracy_score(y_test, y_pred)
            metric_name = "Pr√©cision"
        else:
            self.model = RandomForestRegressor(n_estimators=100, random_state=42)
            self.model.fit(X_train, y_train)
            y_pred = self.model.predict(X_test)
            score = np.sqrt(mean_squared_error(y_test, y_pred))
            metric_name = "RMSE"
        
        print(f"üèÜ Mod√®le s√©lectionn√© : Random Forest")
        print(f"üéØ {metric_name} : {score:.4f}")
        
        # Importance des features
        if hasattr(self.model, 'feature_importances_'):
            feature_importance = pd.DataFrame({
                'feature': X.columns,
                'importance': self.model.feature_importances_
            }).sort_values('importance', ascending=False)
            
            print(f"\nüîë Top 5 features importantes :")
            for idx, row in feature_importance.head().iterrows():
                print(f"   ‚Ä¢ {row['feature']}: {row['importance']:.3f}")
        
        # Sauvegarder les r√©sultats
        self.results = {
            'model_type': 'Random Forest',
            'problem_type': problem_type,
            'score': score,
            'metric': metric_name,
            'feature_importance': feature_importance if hasattr(self.model, 'feature_importances_') else None,
            'encoders': encoders
        }
        
        return self.results
    
    def generate_insights(self):
        """G√©n√©rer des insights automatiquement"""
        print(f"\nüí° INSIGHTS AUTOMATIQUES")
        print("-" * 40)
        
        if not self.results:
            print("‚ùå Aucun mod√®le entra√Æn√© !")
            return
        
        insights = []
        
        # Analyse de performance
        score = self.results['score']
        if self.results['problem_type'] == 'classification':
            if score > 0.9:
                insights.append(f"üéØ Excellente pr√©cision ({score:.1%}) - Mod√®le pr√™t pour production")
            elif score > 0.8:
                insights.append(f"üìä Bonne pr√©cision ({score:.1%}) - Mod√®le fiable")
            else:
                insights.append(f"‚ö†Ô∏è Pr√©cision mod√©r√©e ({score:.1%}) - √Ä am√©liorer")
        
        # Top features
        if self.results['feature_importance'] is not None:
            top_feature = self.results['feature_importance'].iloc[0]
            insights.append(f"üîë Feature la plus importante : {top_feature['feature']} ({top_feature['importance']:.1%})")
        
        # Recommandations
        insights.append("‚úÖ Mod√®le Random Forest s√©lectionn√© (robuste et interpr√©table)")
        insights.append("üìä Pr√™t pour d√©ploiement ou analyse plus pouss√©e")
        
        print("üîç Insights d√©couverts :")
        for insight in insights:
            print(f"   {insight}")
        
        return insights
    
    def save_model(self, filename="model.pkl"):
        """Sauvegarder le mod√®le"""
        if self.model is None:
            print("‚ùå Aucun mod√®le √† sauvegarder !")
            return
        
        import pickle
        
        # Cr√©er le dossier models s'il n'existe pas
        import os
        os.makedirs('models', exist_ok=True)
        
        filepath = f"models/{filename}"
        with open(filepath, 'wb') as f:
            pickle.dump({'model': self.model, 'results': self.results}, f)
        
        print(f"üíæ Mod√®le sauvegard√© : {filepath}")
    
    def create_simple_visualization(self):
        """Cr√©er des visualisations simples"""
        print(f"\nüìà CR√âATION DE VISUALISATIONS")
        print("-" * 40)
        
        if self.data is None:
            print("‚ùå Aucune donn√©e pour visualiser !")
            return
        
        import matplotlib.pyplot as plt
        
        # Cr√©er le dossier visualizations
        import os
        os.makedirs('visualizations', exist_ok=True)
        
        # Graphique 1: Distribution des variables num√©riques
        numeric_columns = self.data.select_dtypes(include=[np.number]).columns
        if len(numeric_columns) > 0:
            fig, axes = plt.subplots(2, 2, figsize=(12, 8))
            axes = axes.ravel()
            
            for i, col in enumerate(numeric_columns[:4]):
                if i < len(axes):
                    axes[i].hist(self.data[col].dropna(), bins=30, alpha=0.7)
                    axes[i].set_title(f'Distribution de {col}')
                    axes[i].set_xlabel(col)
                    axes[i].set_ylabel('Fr√©quence')
            
            plt.tight_layout()
            plt.savefig('visualizations/distributions.png', dpi=300, bbox_inches='tight')
            print("‚úÖ Graphique des distributions sauvegard√©")
            plt.show()
        
        # Graphique 2: Feature Importance
        if self.results and self.results['feature_importance'] is not None:
            plt.figure(figsize=(10, 6))
            top_features = self.results['feature_importance'].head(10)
            plt.barh(range(len(top_features)), top_features['importance'])
            plt.yticks(range(len(top_features)), top_features['feature'])
            plt.xlabel('Importance')
            plt.title('Top 10 Features Importantes')
            plt.tight_layout()
            plt.savefig('visualizations/feature_importance.png', dpi=300, bbox_inches='tight')
            print("‚úÖ Graphique d'importance des features sauvegard√©")
            plt.show()

def main():
    """Fonction principale - COMMENCE ICI !"""
    print("üöÄ D√âMARRAGE DE L'AGENT DATA SCIENTIST AUTONOME")
    print("=" * 60)
    
    # Cr√©er l'agent
    agent = SimpleAutonomousDataScientist()
    
    # √âTAPE 1: Charger tes donn√©es
    print("\nüìÅ √âTAPE 1: Chargement des donn√©es")
    print("T√©l√©charge le dataset Telco Customer Churn depuis Kaggle")
    print("Place le fichier dans le dossier 'data/'")
    
    # Remplace par le chemin vers ton fichier
    data_path = "C:\\Users\\Selim\\OneDrive\\Bureau\\data science agent\\data\\data.xlsx"
    
    # V√©rifier si le fichier existe
    import os
    if not os.path.exists(data_path):
        print(f"‚ùå Fichier non trouv√© : {data_path}")
        print("üì• Cr√©ation de donn√©es d'exemple pour d√©monstration...")
        
        # Cr√©er des donn√©es d'exemple
        os.makedirs('data', exist_ok=True)
        example_data = pd.DataFrame({
            'customer_id': range(1, 1001),
            'age': np.random.normal(40, 12, 1000),
            'income': np.random.normal(50000, 15000, 1000),
            'months_subscribed': np.random.randint(1, 60, 1000),
            'support_calls': np.random.poisson(2, 1000),
            'contract_type': np.random.choice(['Monthly', 'Yearly'], 1000),
            'payment_method': np.random.choice(['Credit Card', 'Bank Transfer', 'Electronic'], 1000),
            'churn': np.random.choice(['Yes', 'No'], 1000, p=[0.3, 0.7])
        })
        
        example_data.to_csv(data_path, index=False)
        print(f"‚úÖ Donn√©es d'exemple cr√©√©es : {data_path}")
    
    # Charger les donn√©es
    if agent.load_data(data_path):
        
        # √âTAPE 2: Analyser la qualit√©
        agent.analyze_data_quality()
        
        # √âTAPE 3: Nettoyer automatiquement
        agent.auto_clean_data()
        
        # √âTAPE 4: Entra√Æner le mod√®le automatiquement
        target_column = 'churn'  # Change selon ton dataset
        results = agent.auto_model_selection(target_column)
        
        if results:
            # √âTAPE 5: G√©n√©rer des insights
            insights = agent.generate_insights()
            
            # √âTAPE 6: Sauvegarder
            agent.save_model()
            
            # √âTAPE 7: Visualisations
            agent.create_simple_visualization()
            
            print("\nüéâ ANALYSE TERMIN√âE AVEC SUCC√àS !")
            print("=" * 60)
            print("üìÅ Fichiers cr√©√©s :")
            print("   ‚Ä¢ models/model.pkl (mod√®le sauvegard√©)")
            print("   ‚Ä¢ visualizations/ (graphiques)")
            
            print(f"\nüìä R√©sum√© :")
            print(f"   ‚Ä¢ Mod√®le : {results['model_type']}")
            print(f"   ‚Ä¢ Performance : {results['score']:.4f}")
            print(f"   ‚Ä¢ Pr√™t pour utilisation !")

if __name__ == "__main__":
    main()